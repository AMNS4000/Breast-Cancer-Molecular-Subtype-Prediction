{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import SimpleITK as sitk\n",
    "from utils import normalize_sitk_image, MRI_Dataset_within_ROI, MRI_Dataset_within_ROI_both_prepost\n",
    "from torchvision.transforms import v2\n",
    "import torchio as tio\n",
    "from torchmetrics import Accuracy, Recall, Precision, F1Score\n",
    "torch.set_num_threads(24)\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SIZE = (100,100,100)\n",
    "TRAIN_SPLIT = 0.75\n",
    "SPLIT_SEED = 123456\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "DATA_PATH = '../../../Processed NIFTI Dataset/'\n",
    "CLASSIFICATION = 'ER'\n",
    "dataset_path = f'../../Train Test Splits/{CLASSIFICATION}/'\n",
    "\n",
    "MODEL_SAVE_PATH = f'basic_model_{CLASSIFICATION}.pth'\n",
    "PROJECT_NAME = 'Breast Cancer Subtype Prediction'\n",
    "\n",
    "NUM_WORKERS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "\n",
    "train_set = pd.read_csv(dataset_path + 'train.csv')\n",
    "test_set  = pd.read_csv(dataset_path + 'test.csv')\n",
    "\n",
    "bounding_boxes = pd.read_csv('../../Data/segmentation_annotations_NIFTI.csv').set_index('Patient_ID')\n",
    "\n",
    "NUM_CLASSES = len(train_set['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_shape_Elongation</th>\n",
       "      <th>original_shape_Flatness</th>\n",
       "      <th>original_shape_LeastAxisLength</th>\n",
       "      <th>original_shape_MajorAxisLength</th>\n",
       "      <th>original_shape_Maximum2DDiameterColumn</th>\n",
       "      <th>original_shape_Maximum2DDiameterRow</th>\n",
       "      <th>original_shape_Maximum2DDiameterSlice</th>\n",
       "      <th>original_shape_Maximum3DDiameter</th>\n",
       "      <th>original_shape_MeshVolume</th>\n",
       "      <th>...</th>\n",
       "      <th>original_glszm_SmallAreaLowGrayLevelEmphasis</th>\n",
       "      <th>original_glszm_ZoneEntropy</th>\n",
       "      <th>original_glszm_ZonePercentage</th>\n",
       "      <th>original_glszm_ZoneVariance</th>\n",
       "      <th>original_ngtdm_Busyness</th>\n",
       "      <th>original_ngtdm_Coarseness</th>\n",
       "      <th>original_ngtdm_Complexity</th>\n",
       "      <th>original_ngtdm_Contrast</th>\n",
       "      <th>original_ngtdm_Strength</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>0.962565</td>\n",
       "      <td>0.853207</td>\n",
       "      <td>12.531504</td>\n",
       "      <td>14.687523</td>\n",
       "      <td>16.001639</td>\n",
       "      <td>16.504796</td>\n",
       "      <td>17.428449</td>\n",
       "      <td>20.185653</td>\n",
       "      <td>1655.774227</td>\n",
       "      <td>...</td>\n",
       "      <td>7.903733e-08</td>\n",
       "      <td>-3.203427e-16</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>777</td>\n",
       "      <td>0.795114</td>\n",
       "      <td>0.575234</td>\n",
       "      <td>41.275598</td>\n",
       "      <td>71.754477</td>\n",
       "      <td>81.554735</td>\n",
       "      <td>89.356894</td>\n",
       "      <td>106.275846</td>\n",
       "      <td>109.381063</td>\n",
       "      <td>80893.671683</td>\n",
       "      <td>...</td>\n",
       "      <td>4.901414e-01</td>\n",
       "      <td>2.902818e+00</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>6.014770e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>580</td>\n",
       "      <td>0.674057</td>\n",
       "      <td>0.509993</td>\n",
       "      <td>20.299251</td>\n",
       "      <td>39.802964</td>\n",
       "      <td>32.193327</td>\n",
       "      <td>46.356854</td>\n",
       "      <td>49.344909</td>\n",
       "      <td>50.722644</td>\n",
       "      <td>9592.051652</td>\n",
       "      <td>...</td>\n",
       "      <td>5.530019e-01</td>\n",
       "      <td>2.300814e+00</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>8.841581e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>858</td>\n",
       "      <td>0.460551</td>\n",
       "      <td>0.364788</td>\n",
       "      <td>19.775146</td>\n",
       "      <td>54.209994</td>\n",
       "      <td>30.968839</td>\n",
       "      <td>49.615171</td>\n",
       "      <td>49.696789</td>\n",
       "      <td>53.641526</td>\n",
       "      <td>9223.914005</td>\n",
       "      <td>...</td>\n",
       "      <td>6.344686e-01</td>\n",
       "      <td>2.249814e+00</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>6.205718e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>538</td>\n",
       "      <td>0.777690</td>\n",
       "      <td>0.651157</td>\n",
       "      <td>44.137145</td>\n",
       "      <td>67.782618</td>\n",
       "      <td>74.346792</td>\n",
       "      <td>77.918124</td>\n",
       "      <td>64.973284</td>\n",
       "      <td>87.819714</td>\n",
       "      <td>61198.814132</td>\n",
       "      <td>...</td>\n",
       "      <td>5.570141e-01</td>\n",
       "      <td>2.474841e+00</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>1.139299e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>625</td>\n",
       "      <td>0.843551</td>\n",
       "      <td>0.784001</td>\n",
       "      <td>13.959376</td>\n",
       "      <td>17.805311</td>\n",
       "      <td>17.807484</td>\n",
       "      <td>20.334814</td>\n",
       "      <td>20.109152</td>\n",
       "      <td>23.239152</td>\n",
       "      <td>2257.450217</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>4.361832e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>643</td>\n",
       "      <td>0.863104</td>\n",
       "      <td>0.593432</td>\n",
       "      <td>17.211454</td>\n",
       "      <td>29.003257</td>\n",
       "      <td>29.337226</td>\n",
       "      <td>26.632793</td>\n",
       "      <td>33.811070</td>\n",
       "      <td>36.594924</td>\n",
       "      <td>7328.440348</td>\n",
       "      <td>...</td>\n",
       "      <td>3.125000e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>3.606603e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>543</td>\n",
       "      <td>0.436385</td>\n",
       "      <td>0.419528</td>\n",
       "      <td>24.462222</td>\n",
       "      <td>58.308908</td>\n",
       "      <td>30.586967</td>\n",
       "      <td>54.975914</td>\n",
       "      <td>55.109923</td>\n",
       "      <td>58.939915</td>\n",
       "      <td>22617.573935</td>\n",
       "      <td>...</td>\n",
       "      <td>8.199772e-10</td>\n",
       "      <td>-3.203427e-16</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>351</td>\n",
       "      <td>0.782644</td>\n",
       "      <td>0.633827</td>\n",
       "      <td>14.731498</td>\n",
       "      <td>23.242127</td>\n",
       "      <td>23.899082</td>\n",
       "      <td>20.694753</td>\n",
       "      <td>26.035769</td>\n",
       "      <td>28.726272</td>\n",
       "      <td>3826.559124</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466667e-01</td>\n",
       "      <td>1.584963e+00</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>1.384783e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>807</td>\n",
       "      <td>0.600852</td>\n",
       "      <td>0.522099</td>\n",
       "      <td>81.883509</td>\n",
       "      <td>156.835258</td>\n",
       "      <td>108.163355</td>\n",
       "      <td>155.110760</td>\n",
       "      <td>160.281779</td>\n",
       "      <td>174.621702</td>\n",
       "      <td>681740.842467</td>\n",
       "      <td>...</td>\n",
       "      <td>6.116462e-01</td>\n",
       "      <td>1.945251e+00</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>1.405885e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>645 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  original_shape_Elongation  original_shape_Flatness  \\\n",
       "0            92                   0.962565                 0.853207   \n",
       "1           777                   0.795114                 0.575234   \n",
       "2           580                   0.674057                 0.509993   \n",
       "3           858                   0.460551                 0.364788   \n",
       "4           538                   0.777690                 0.651157   \n",
       "..          ...                        ...                      ...   \n",
       "640         625                   0.843551                 0.784001   \n",
       "641         643                   0.863104                 0.593432   \n",
       "642         543                   0.436385                 0.419528   \n",
       "643         351                   0.782644                 0.633827   \n",
       "644         807                   0.600852                 0.522099   \n",
       "\n",
       "     original_shape_LeastAxisLength  original_shape_MajorAxisLength  \\\n",
       "0                         12.531504                       14.687523   \n",
       "1                         41.275598                       71.754477   \n",
       "2                         20.299251                       39.802964   \n",
       "3                         19.775146                       54.209994   \n",
       "4                         44.137145                       67.782618   \n",
       "..                              ...                             ...   \n",
       "640                       13.959376                       17.805311   \n",
       "641                       17.211454                       29.003257   \n",
       "642                       24.462222                       58.308908   \n",
       "643                       14.731498                       23.242127   \n",
       "644                       81.883509                      156.835258   \n",
       "\n",
       "     original_shape_Maximum2DDiameterColumn  \\\n",
       "0                                 16.001639   \n",
       "1                                 81.554735   \n",
       "2                                 32.193327   \n",
       "3                                 30.968839   \n",
       "4                                 74.346792   \n",
       "..                                      ...   \n",
       "640                               17.807484   \n",
       "641                               29.337226   \n",
       "642                               30.586967   \n",
       "643                               23.899082   \n",
       "644                              108.163355   \n",
       "\n",
       "     original_shape_Maximum2DDiameterRow  \\\n",
       "0                              16.504796   \n",
       "1                              89.356894   \n",
       "2                              46.356854   \n",
       "3                              49.615171   \n",
       "4                              77.918124   \n",
       "..                                   ...   \n",
       "640                            20.334814   \n",
       "641                            26.632793   \n",
       "642                            54.975914   \n",
       "643                            20.694753   \n",
       "644                           155.110760   \n",
       "\n",
       "     original_shape_Maximum2DDiameterSlice  original_shape_Maximum3DDiameter  \\\n",
       "0                                17.428449                         20.185653   \n",
       "1                               106.275846                        109.381063   \n",
       "2                                49.344909                         50.722644   \n",
       "3                                49.696789                         53.641526   \n",
       "4                                64.973284                         87.819714   \n",
       "..                                     ...                               ...   \n",
       "640                              20.109152                         23.239152   \n",
       "641                              33.811070                         36.594924   \n",
       "642                              55.109923                         58.939915   \n",
       "643                              26.035769                         28.726272   \n",
       "644                             160.281779                        174.621702   \n",
       "\n",
       "     original_shape_MeshVolume  ...  \\\n",
       "0                  1655.774227  ...   \n",
       "1                 80893.671683  ...   \n",
       "2                  9592.051652  ...   \n",
       "3                  9223.914005  ...   \n",
       "4                 61198.814132  ...   \n",
       "..                         ...  ...   \n",
       "640                2257.450217  ...   \n",
       "641                7328.440348  ...   \n",
       "642               22617.573935  ...   \n",
       "643                3826.559124  ...   \n",
       "644              681740.842467  ...   \n",
       "\n",
       "     original_glszm_SmallAreaLowGrayLevelEmphasis  original_glszm_ZoneEntropy  \\\n",
       "0                                    7.903733e-08               -3.203427e-16   \n",
       "1                                    4.901414e-01                2.902818e+00   \n",
       "2                                    5.530019e-01                2.300814e+00   \n",
       "3                                    6.344686e-01                2.249814e+00   \n",
       "4                                    5.570141e-01                2.474841e+00   \n",
       "..                                            ...                         ...   \n",
       "640                                  5.000000e-01                1.000000e+00   \n",
       "641                                  3.125000e-02                1.000000e+00   \n",
       "642                                  8.199772e-10               -3.203427e-16   \n",
       "643                                  3.466667e-01                1.584963e+00   \n",
       "644                                  6.116462e-01                1.945251e+00   \n",
       "\n",
       "     original_glszm_ZonePercentage  original_glszm_ZoneVariance  \\\n",
       "0                         0.000281                 0.000000e+00   \n",
       "1                         0.001373                 6.014770e+07   \n",
       "2                         0.001829                 8.841581e+06   \n",
       "3                         0.002600                 6.205718e+06   \n",
       "4                         0.001206                 1.139299e+08   \n",
       "..                             ...                          ...   \n",
       "640                       0.000479                 4.361832e+06   \n",
       "641                       0.000166                 3.606603e+07   \n",
       "642                       0.000029                 0.000000e+00   \n",
       "643                       0.000380                 1.384783e+07   \n",
       "644                       0.000064                 1.405885e+10   \n",
       "\n",
       "     original_ngtdm_Busyness  original_ngtdm_Coarseness  \\\n",
       "0                        0.0                  1000000.0   \n",
       "1                        0.0                  1000000.0   \n",
       "2                        0.0                  1000000.0   \n",
       "3                        0.0                  1000000.0   \n",
       "4                        0.0                  1000000.0   \n",
       "..                       ...                        ...   \n",
       "640                      0.0                  1000000.0   \n",
       "641                      0.0                  1000000.0   \n",
       "642                      0.0                  1000000.0   \n",
       "643                      0.0                  1000000.0   \n",
       "644                      0.0                  1000000.0   \n",
       "\n",
       "     original_ngtdm_Complexity  original_ngtdm_Contrast  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "..                         ...                      ...   \n",
       "640                        0.0                      0.0   \n",
       "641                        0.0                      0.0   \n",
       "642                        0.0                      0.0   \n",
       "643                        0.0                      0.0   \n",
       "644                        0.0                      0.0   \n",
       "\n",
       "     original_ngtdm_Strength  label  \n",
       "0                        0.0      1  \n",
       "1                        0.0      0  \n",
       "2                        0.0      1  \n",
       "3                        0.0      1  \n",
       "4                        0.0      1  \n",
       "..                       ...    ...  \n",
       "640                      0.0      1  \n",
       "641                      0.0      1  \n",
       "642                      0.0      1  \n",
       "643                      0.0      1  \n",
       "644                      0.0      1  \n",
       "\n",
       "[645 rows x 109 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set split\n",
    "\n",
    "train_set, val_set = train_test_split(train_set, train_size = TRAIN_SPLIT, stratify = train_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "\n",
    "transform = v2.Compose([\n",
    "    tio.ZNormalization()\n",
    "    # v2.Normalize(mean = [0.5], std = [0.225])\n",
    "])\n",
    "\n",
    "upscaler = tio.Resize(SIZE, 'bspline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_args = {\n",
    "        'scales' : [0.8, 1, 0.8, 1, 0.8, 1],\n",
    "        'degrees' : 15,\n",
    "        'translation' : [0.05, 0.05, 0.05],\n",
    "        'center' : 'image'\n",
    "}\n",
    "augment = tio.Compose([\n",
    "    tio.transforms.RandomAffine(\n",
    "        **affine_args\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = train_set.label.value_counts()  \n",
    "class_sample_count = 1/class_sample_count\n",
    "class_sample_count = class_sample_count.tolist()\n",
    "\n",
    "c_w = train_set.label.apply(lambda x: class_sample_count[x]).to_numpy()\n",
    "c_w = torch.tensor(c_w, dtype = torch.double)\n",
    "\n",
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(c_w, len(c_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483, 109)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "\n",
    "\n",
    "def normalize_sitk_image(arr):\n",
    "    '''Function to scale a simple-itk image array to values between 0 and 1\n",
    "    \n",
    "    Arguments:\n",
    "    1. arr: a numpy array for a SimpleITK image\n",
    "    \n",
    "    Returns:\n",
    "    1. scaled_arr: a scaled array with elements between 0 and 1.'''\n",
    "\n",
    "    return (arr - arr.min())/(arr.max() - arr.min())\n",
    "\n",
    "def convert(n):\n",
    "    n = int(n)\n",
    "    n+=1\n",
    "    if(n>=0 and n<10):\n",
    "        return \"00\"+str(int(n))\n",
    "    \n",
    "    elif(n>=10 and n<100):\n",
    "        return \"0\"+str(int(n))\n",
    "    \n",
    "    return str(int(n))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class MRI_Dataset_within_ROI(torch.utils.data.Dataset):\n",
    "    '''Dataset for loading MRI sequences with only the tumour ROI enclosed.'''\n",
    "    def __init__(self,\n",
    "                 src_path,\n",
    "                 dataframe,\n",
    "                 seg_bb,\n",
    "                 transform,\n",
    "                 upscale,\n",
    "                 augment = None,\n",
    "                 sequence = 'post_1.img.gz'\n",
    "                 ):\n",
    "        '''Init method\n",
    "\n",
    "        Arguments:\n",
    "        1. src_path: the path to the processed NIFTI Dataset\n",
    "        2. dataframe: the file consisting of patient-class characterisations\n",
    "        3. seg_bb: segmentation bounding boxes data (dataframe)\n",
    "        4. transform: the transformation excluding the upscaling for the 3D volume\n",
    "        5. upscale: the upscaling transform to convert sequences to a standard format\n",
    "        6. augment: augmentation for the 3D voxel tensor\n",
    "        7. sequence: the sequence name (eg. 'post_1.img.gz')\n",
    "\n",
    "        Returns:\n",
    "        1. MRI_Dataset_within_ROI dataset\n",
    "        '''\n",
    "        self.src_path = src_path\n",
    "        self.df = dataframe.to_numpy()\n",
    "        self.seg_bb = seg_bb\n",
    "        self.transform = transform\n",
    "        self.sequence = sequence\n",
    "        self.upscale = upscale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Function to get the length of the dataset'''\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Function to fetch item at an index of the dataset\n",
    "\n",
    "        Arguments:\n",
    "        1. idx: index\n",
    "\n",
    "        Returns:\n",
    "        1. 3D tensor for the tumour volume\n",
    "        2. label\n",
    "        '''\n",
    "        patient, label = self.df[idx][0],self.df[idx][-1]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        path = os.path.join(self.src_path, \"Breast_MRI_\"+convert(patient), self.sequence)\n",
    "        img = sitk.ReadImage(path)\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        row1, row2, col1, col2, slice1, slice2 = self.seg_bb.loc[\"Breast_MRI_\"+convert(patient)].tolist()\n",
    "\n",
    "        segment = torch.tensor(arr[slice1: slice2, row1: row2, col1: col2].astype('float32'))[None, ...]\n",
    "        segment = self.transform(segment).type(torch.float32)\n",
    "    \n",
    "        segment = self.upscale(segment)\n",
    "        if self.augment is not None:\n",
    "            segment = self.augment(segment)\n",
    "\n",
    "        label = label.type(torch.LongTensor)\n",
    "        \n",
    "        return segment, label\n",
    "    \n",
    "    \n",
    "class MRI_Dataset_within_ROI_both_prepost(torch.utils.data.Dataset):\n",
    "    '''Dataset for loading MRI sequences with only the tumour ROI enclosed.'''\n",
    "    def __init__(self,\n",
    "                 src_path,\n",
    "                 dataframe,\n",
    "                 seg_bb,\n",
    "                 transform,\n",
    "                 upscale,\n",
    "                 augment = None\n",
    "                 ):\n",
    "        '''Init method\n",
    "\n",
    "        Arguments:\n",
    "        1. src_path: the path to the processed NIFTI Dataset\n",
    "        2. dataframe: the file consisting of patient-class characterisations\n",
    "        3. seg_bb: segmentation bounding boxes data (dataframe)\n",
    "        4. transform: the transformation excluding the upscaling for the 3D volume\n",
    "        5. upscale: the upscaling transform to convert sequences to a standard format\n",
    "        6. augment: augmentation for the 3D voxel tensor\n",
    "\n",
    "        Returns:\n",
    "        1. MRI_Dataset_within_ROI dataset\n",
    "        '''\n",
    "        self.src_path = src_path\n",
    "        self.df = dataframe.to_numpy()\n",
    "        self.seg_bb = seg_bb\n",
    "        self.transform = transform\n",
    "        self.upscale = upscale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Function to get the length of the dataset'''\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Function to fetch item at an index of the dataset\n",
    "\n",
    "        Arguments:\n",
    "        1. idx: index\n",
    "\n",
    "        Returns:\n",
    "        1. 3D tensor for the tumour volume\n",
    "        2. label\n",
    "        '''\n",
    "        patient, label = self.df[idx][0],self.df[idx][-1]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        pre_path = os.path.join(self.src_path, \"Breast_MRI_\"+convert(patient), 'pre.img.gz')\n",
    "        post_path = os.path.join(self.src_path, \"Breast_MRI_\"+convert(patient), 'post_1.img.gz')\n",
    "\n",
    "    \n",
    "        row1, row2, col1, col2, slice1, slice2 = self.seg_bb.loc[\"Breast_MRI_\"+convert(patient)].tolist()\n",
    "        pre_img = self.get_arrs(pre_path, row1, row2, col1, col2, slice1, slice2)\n",
    "        post_img = self.get_arrs(post_path, row1, row2, col1, col2, slice1, slice2)\n",
    "\n",
    "        \n",
    "        segment = torch.concat([pre_img, post_img])\n",
    "        \n",
    "        if self.augment is not None:\n",
    "            segment = self.augment(segment)\n",
    "\n",
    "        label = label.type(torch.LongTensor)\n",
    "        \n",
    "        return segment, label\n",
    "    \n",
    "\n",
    "    def get_arrs(self, path, row1, row2, col1, col2, slice1, slice2):\n",
    "        img = sitk.GetArrayFromImage(sitk.ReadImage(path))[slice1: slice2, row1: row2, col1: col2]\n",
    "        img = torch.Tensor(img.astype('float32'))[None, ...]\n",
    "        return self.upscale(self.transform(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MRI_Dataset_within_ROI_both_prepost(DATA_PATH,\n",
    "                                       train_set,\n",
    "                                       bounding_boxes, \n",
    "                                       transform,\n",
    "                                       upscaler,\n",
    "                                       augment = augment,\n",
    "                                       )\n",
    "\n",
    "val_dataset = MRI_Dataset_within_ROI_both_prepost(DATA_PATH,\n",
    "                                       val_set,\n",
    "                                       bounding_boxes, \n",
    "                                       transform,\n",
    "                                       upscaler)\n",
    "\n",
    "test_dataset = MRI_Dataset_within_ROI_both_prepost(DATA_PATH,\n",
    "                                       test_set,\n",
    "                                       bounding_boxes, \n",
    "                                       transform,\n",
    "                                       upscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483, 109)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Start Row       234\n",
       "End Row         271\n",
       "Start Column    308\n",
       "End Column      341\n",
       "Start Slice      89\n",
       "End Slice       112\n",
       "Name: Breast_MRI_001, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_cnn = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        #    shuffle = True,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           pin_memory = True,\n",
    "                                           num_workers = NUM_WORKERS,\n",
    "                                           sampler = weighted_sampler,\n",
    "                                           persistent_workers = True\n",
    "                                           )\n",
    "\n",
    "val_loader_cnn = torch.utils.data.DataLoader(val_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           pin_memory = True,\n",
    "                                           num_workers = NUM_WORKERS,\n",
    "                                           persistent_workers = True\n",
    "                                           )\n",
    "\n",
    "test_loader_cnn = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           pin_memory = True,\n",
    "                                           num_workers = NUM_WORKERS,\n",
    "                                           persistent_workers = True\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet_MRI3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ConvNet_MRI3D, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv2 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv3 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv4 = torch.nn.Conv3d(16, 32, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        \n",
    "        # self.conv1x1_1 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        # self.conv1x1_2 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        self.maxpool1 = torch.nn.MaxPool3d(2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(3456*2, num_classes)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = F.relu(self.conv1(out))\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        \n",
    "        out = F.relu(self.conv2(intermediate))\n",
    "        out = out + intermediate      # residual\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv3(intermediate))\n",
    "        out = out + intermediate\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2567, 0.7433])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor((train_set['label'].value_counts()/len(train_set)).to_numpy())\n",
    "weights = 1 - weights\n",
    "weights = weights.float()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_set_tensor = torch.tensor(train_set.iloc[:, :-1].values, dtype=torch.float32)  # Excluding the last column (labels)\n",
    "train_labels_tensor = torch.tensor(train_set.iloc[:, -1].values, dtype=torch.long)  # Extracting the labels\n",
    "\n",
    "# Assuming val_set and test_set are similar DataFrames\n",
    "val_set_tensor = torch.tensor(val_set.iloc[:, :-1].values, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_set.iloc[:, -1].values, dtype=torch.long)\n",
    "\n",
    "test_set_tensor = torch.tensor(test_set.iloc[:, :-1].values, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_set.iloc[:, -1].values, dtype=torch.long)\n",
    "batch_size = 25\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_set_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_set_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_set_tensor, test_labels_tensor)\n",
    "train_loader_lstm = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_lstm = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader_lstm = DataLoader(test_dataset, batch_size=batch_size)\n",
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4569, Train Acc: 0.8716, Val Loss: 0.5815, Val Acc: 0.7469\n",
      "Epoch 2/10, Train Loss: 0.3543, Train Acc: 0.9130, Val Loss: 0.5799, Val Acc: 0.7469\n",
      "Epoch 3/10, Train Loss: 0.3659, Train Acc: 0.8861, Val Loss: 0.5748, Val Acc: 0.7469\n",
      "Epoch 4/10, Train Loss: 0.3644, Train Acc: 0.8861, Val Loss: 0.5802, Val Acc: 0.7469\n",
      "Epoch 5/10, Train Loss: 0.3624, Train Acc: 0.8820, Val Loss: 0.5920, Val Acc: 0.7469\n",
      "Epoch 6/10, Train Loss: 0.3765, Train Acc: 0.8737, Val Loss: 0.5743, Val Acc: 0.7469\n",
      "Epoch 7/10, Train Loss: 0.4179, Train Acc: 0.8509, Val Loss: 0.5956, Val Acc: 0.7469\n",
      "Epoch 8/10, Train Loss: 0.3351, Train Acc: 0.8965, Val Loss: 0.6404, Val Acc: 0.7407\n",
      "Epoch 9/10, Train Loss: 0.3170, Train Acc: 0.8986, Val Loss: 0.6014, Val Acc: 0.7407\n",
      "Epoch 10/10, Train Loss: 0.3104, Train Acc: 0.8986, Val Loss: 0.6366, Val Acc: 0.7099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (101, 100, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6323, Test Acc: 0.7329\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Ensure both tensors have the same batch size\n",
    "        batch_size = hidden.size(0)\n",
    "        encoder_outputs = encoder_outputs[:, :batch_size, :]  # Adjust encoder_outputs if needed\n",
    "        \n",
    "        # Add a singleton dimension to encoder_outputs\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(2)  # Add a singleton dimension\n",
    "\n",
    "        # Concatenate the hidden state with encoder outputs along the last dimension\n",
    "        combined = torch.cat((hidden.unsqueeze(1), encoder_outputs), dim=-1)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "\n",
    "        # Squeeze the attention scores to remove the added singleton dimension\n",
    "        energy = energy.squeeze(2)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = F.softmax(torch.matmul(energy, self.v), dim=1)\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        attended_encoder_outputs = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return attention_weights, attended_encoder_outputs\n",
    "\n",
    "\n",
    "# Define the combined model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, lstm_model, conv_model, hidden_size_lstm, num_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.lstm_model = lstm_model\n",
    "        self.conv_model = conv_model\n",
    "        self.fc1 = nn.Linear(hidden_size_lstm + num_classes, 256)  # Adjusted size for fc1\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_conv):\n",
    "        out_lstm = self.lstm_model(x_lstm)\n",
    "        out_conv = self.conv_model(x_conv)\n",
    "\n",
    "        # Adjust the size of out_lstm to match the second dimension of out_conv\n",
    "        out_lstm = out_lstm[:, :out_conv.size(1)]  # Trim out_lstm if needed\n",
    "        \n",
    "        # Concatenate the LSTM output and ConvNet output\n",
    "        combined_representation = torch.cat((out_lstm, out_conv), dim=1)\n",
    "        # Pass the combined representation through the fully connected layers\n",
    "        out = F.relu(self.fc1(combined_representation))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Define the ConvNet model\n",
    "class ConvNet_MRI3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ConvNet_MRI3D, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv2 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv3 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv4 = torch.nn.Conv3d(16, 32, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        \n",
    "        # self.conv1x1_1 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        # self.conv1x1_2 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        self.maxpool1 = torch.nn.MaxPool3d(2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(3456*2, num_classes)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = F.relu(self.conv1(out))\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        \n",
    "        out = F.relu(self.conv2(intermediate))\n",
    "        out = out + intermediate      # residual\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv3(intermediate))\n",
    "        out = out + intermediate\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the LSTM model\n",
    "input_size_lstm = train_set_tensor.shape[1]  # Update this according to your input size\n",
    "hidden_size_lstm = 64  # Change this according to your desired size\n",
    "output_size_lstm = 2  # Change this according to your output size\n",
    "lstm_model = LSTMModel(input_size_lstm, hidden_size_lstm, output_size_lstm)\n",
    "\n",
    "# Instantiate the ConvNet model\n",
    "in_channels_conv = 2  # Update this according to your input channels\n",
    "num_classes_conv = 2  # Change this according to your output size\n",
    "conv_model = ConvNet_MRI3D(in_channels_conv, num_classes_conv)\n",
    "\n",
    "# Instantiate the combined model\n",
    "combined_model = CombinedModel(lstm_model, conv_model,num_classes_conv)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(train_loader_lstm, train_loader_cnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader_lstm.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for (data_lstm, _), (data_conv, labels) in zip(val_loader_lstm, val_loader_cnn):\n",
    "            outputs = combined_model(data_lstm, data_conv)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted_val = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted_val == labels).sum().item()\n",
    "    val_loss /= len(val_loader_lstm.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Testing\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(test_loader_lstm, test_loader_cnn):\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_test = torch.max(outputs, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted_test == labels).sum().item()\n",
    "test_loss /= len(test_loader_lstm.dataset)\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(combined_model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (101, 100, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (1, 100, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5106, Train Acc: 0.7930, Val Loss: 0.6486, Val Acc: 0.6481\n",
      "Epoch 2/10, Train Loss: 0.5714, Train Acc: 0.7495, Val Loss: 0.6710, Val Acc: 0.6481\n",
      "Epoch 3/10, Train Loss: 0.5304, Train Acc: 0.7888, Val Loss: 0.6652, Val Acc: 0.6481\n",
      "Epoch 4/10, Train Loss: 0.5392, Train Acc: 0.7826, Val Loss: 0.7065, Val Acc: 0.6481\n",
      "Epoch 5/10, Train Loss: 0.4863, Train Acc: 0.8137, Val Loss: 0.7826, Val Acc: 0.6481\n",
      "Epoch 6/10, Train Loss: 0.5347, Train Acc: 0.7805, Val Loss: 0.6986, Val Acc: 0.6481\n",
      "Epoch 7/10, Train Loss: 0.5484, Train Acc: 0.7660, Val Loss: 0.6901, Val Acc: 0.6481\n",
      "Epoch 8/10, Train Loss: 0.5817, Train Acc: 0.7433, Val Loss: 0.6711, Val Acc: 0.6481\n",
      "Epoch 9/10, Train Loss: 0.5288, Train Acc: 0.7805, Val Loss: 0.7344, Val Acc: 0.6481\n",
      "Epoch 10/10, Train Loss: 0.5574, Train Acc: 0.7702, Val Loss: 0.8679, Val Acc: 0.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 101, 100) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n",
      "/home/dhruv.kumar/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torchio/transforms/transform.py:163: RuntimeWarning: Output shape (100, 100, 101) != target shape (100, 100, 100). Fixing with CropOrPad\n",
      "  transformed = self.apply_transform(subject)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8643, Test Acc: 0.6498\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(4, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(25, 4))  # Adjusted to match the dimensions\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Concatenate the hidden state with encoder outputs along the last dimension\n",
    "        combined = torch.cat((hidden, encoder_outputs), dim=-1)\n",
    "        # Calculate the attention scores\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "        # Calculate attention weights\n",
    "        attention_weights = torch.matmul(energy, self.v)\n",
    "        # attended_encoder_outputs = attended_encoder_outputs.transpose(0, 1)  # Transpose back\n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, lstm_model, conv_model, hidden_size_lstm, num_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.lstm_model = lstm_model\n",
    "        self.conv_model = conv_model\n",
    "        self.attention = Attention(hidden_size_lstm)  # Attention mechanism\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_conv):\n",
    "        out_lstm = self.lstm_model(x_lstm)\n",
    "        out_conv = self.conv_model(x_conv)\n",
    "\n",
    "        # Adjust the size of out_lstm to match the second dimension of out_conv\n",
    "        out_lstm = out_lstm[:, :out_conv.size(1)]  # Trim out_lstm if needed\n",
    "        combined_representation = torch.cat((out_lstm, out_conv), dim=1)\n",
    "        # Apply attention mechanism\n",
    "        attended_combined_representation = self.attention(out_lstm, out_conv)\n",
    "        combined_representation = torch.cat((out_lstm, out_conv), dim=1)\n",
    "        # Pass the attended combined representation through the fully connected layers\n",
    "        out = F.relu(self.fc1(attended_combined_representation))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Define the ConvNet model\n",
    "class ConvNet_MRI3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ConvNet_MRI3D, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv2 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv3 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv4 = torch.nn.Conv3d(16, 32, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        \n",
    "        # self.conv1x1_1 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        # self.conv1x1_2 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        self.maxpool1 = torch.nn.MaxPool3d(2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(3456*2, num_classes)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = F.relu(self.conv1(out))\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        \n",
    "        out = F.relu(self.conv2(intermediate))\n",
    "        out = out + intermediate      # residual\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv3(intermediate))\n",
    "        out = out + intermediate\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the LSTM model\n",
    "input_size_lstm = train_set_tensor.shape[1]  # Update this according to your input size\n",
    "hidden_size_lstm = 25  # Change this according to your desired size\n",
    "output_size_lstm = 2  # Change this according to your output size\n",
    "lstm_model = LSTMModel(input_size_lstm, hidden_size_lstm, output_size_lstm)\n",
    "\n",
    "# Instantiate the ConvNet model\n",
    "in_channels_conv = 2  # Update this according to your input channels\n",
    "num_classes_conv = 2  # Change this according to your output size\n",
    "conv_model = ConvNet_MRI3D(in_channels_conv, num_classes_conv)\n",
    "\n",
    "# Instantiate the combined model\n",
    "combined_model = CombinedModel(lstm_model, conv_model, hidden_size_lstm, num_classes_conv)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(train_loader_lstm, train_loader_cnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader_lstm.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for (data_lstm, _), (data_conv, labels) in zip(val_loader_lstm, val_loader_cnn):\n",
    "            outputs = combined_model(data_lstm, data_conv)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted_val = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted_val == labels).sum().item()\n",
    "    val_loss /= len(val_loader_lstm.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Testing\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(test_loader_lstm, test_loader_cnn):\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_test = torch.max(outputs, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted_test == labels).sum().item()\n",
    "test_loss /= len(test_loader_lstm.dataset)\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.branch1x1 = nn.Conv3d(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, red_3x3, kernel_size=1),\n",
    "            nn.Conv3d(red_3x3, out_3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, red_5x5, kernel_size=1),\n",
    "            nn.Conv3d(red_5x5, out_5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # 3x3 max pooling -> 1x1 conv branch\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv3d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class ConvNet_MRI3D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ConvNet_MRI3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1)\n",
    "        self.inception1 = InceptionModule(16, 16, 16, 16, 16, 16, 16)\n",
    "        self.inception2 = InceptionModule(64, 16, 16, 16, 16, 16, 16)\n",
    "        self.inception3 = InceptionModule(64, 32, 32, 32, 32, 32, 32)\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool3d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = F.relu(self.conv1(out))\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out = self.inception1(out)\n",
    "        out = self.inception2(out)\n",
    "        out = self.inception3(out)\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 1 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[32], line 145\u001b[0m\n",
      "\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data_lstm, _), (data_conv, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_loader_lstm, train_loader_cnn):\n",
      "\u001b[1;32m    144\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m--> 145\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_conv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    146\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[1;32m    147\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[32], line 59\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, x_lstm, x_conv)\u001b[0m\n",
      "\u001b[1;32m     56\u001b[0m combined_representation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((out_lstm, out_conv), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Pass the combined representation through the attention mechanism\u001b[39;00m\n",
      "\u001b[0;32m---> 59\u001b[0m attention_weights, attended_combined_representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_representation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_representation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Pass the attended combined representation through the fully connected layers\u001b[39;00m\n",
      "\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(attended_combined_representation))\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/breast_cancer_py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenate the hidden state with encoder outputs along the last dimension\u001b[39;00m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;32m---> 22\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate the attention scores\u001b[39;00m\n",
      "\u001b[1;32m     25\u001b[0m energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(combined))\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Add a singleton dimension to encoder_outputs\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(2)  # Add a singleton dimension\n",
    "\n",
    "        # Concatenate the hidden state with encoder outputs along the last dimension\n",
    "        print(hidden.unsqueeze(1).shape)\n",
    "        combined = torch.cat((hidden.unsqueeze(1), encoder_outputs), dim=-1)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "\n",
    "        # Squeeze the attention scores to remove the added singleton dimension\n",
    "        energy = energy.squeeze(2)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = F.softmax(torch.matmul(energy, self.v), dim=1)\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        attended_encoder_outputs = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return attention_weights, attended_encoder_outputs\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, lstm_model, conv_model, hidden_size_lstm, num_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.lstm_model = lstm_model\n",
    "        self.conv_model = conv_model\n",
    "        self.attention = Attention(hidden_size_lstm)\n",
    "        self.fc1 = nn.Linear(hidden_size_lstm + num_classes, 256)  # Adjusted size for fc1\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_conv):\n",
    "        out_lstm = self.lstm_model(x_lstm)\n",
    "        out_conv = self.conv_model(x_conv)\n",
    "\n",
    "        # Adjust the size of out_lstm to match the second dimension of out_conv\n",
    "        out_lstm = out_lstm[:, :out_conv.size(1)]  # Trim out_lstm if needed\n",
    "        \n",
    "        # Concatenate the LSTM output and ConvNet output\n",
    "        combined_representation = torch.cat((out_lstm, out_conv), dim=1)\n",
    "\n",
    "        # Pass the combined representation through the attention mechanism\n",
    "        attention_weights, attended_combined_representation = self.attention(combined_representation, combined_representation)\n",
    "\n",
    "        # Pass the attended combined representation through the fully connected layers\n",
    "        out = F.relu(self.fc1(attended_combined_representation))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Define the ConvNet model\n",
    "class ConvNet_MRI3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ConvNet_MRI3D, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv2 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv3 = torch.nn.Conv3d(16, 16, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        self.conv4 = torch.nn.Conv3d(16, 32, kernel_size = (3,3,3), stride = (1,1,1), padding = 1)\n",
    "        \n",
    "        # self.conv1x1_1 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        # self.conv1x1_2 = torch.nn.Conv3d(16, 16, kernel_size=1, stride=1)\n",
    "        self.maxpool1 = torch.nn.MaxPool3d(2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(3456*2, num_classes)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        out = F.relu(self.conv1(out))\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        \n",
    "        out = F.relu(self.conv2(intermediate))\n",
    "        out = out + intermediate      # residual\n",
    "        intermediate = self.maxpool1(out)\n",
    "        \n",
    "        out = F.relu(self.conv3(intermediate))\n",
    "        out = out + intermediate\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.flatten(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the LSTM model\n",
    "input_size_lstm = train_set_tensor.shape[1]  # Update this according to your input size\n",
    "hidden_size_lstm = 64  # Change this according to your desired size\n",
    "output_size_lstm = 2  # Change this according to your output size\n",
    "lstm_model = LSTMModel(input_size_lstm, hidden_size_lstm, output_size_lstm)\n",
    "\n",
    "# Instantiate the ConvNet model\n",
    "in_channels_conv = 2  # Update this according to your input channels\n",
    "num_classes_conv = 2  # Change this according to your output size\n",
    "conv_model = ConvNet_MRI3D(in_channels_conv, num_classes_conv)\n",
    "\n",
    "# Instantiate the combined model\n",
    "combined_model = CombinedModel(lstm_model, conv_model, hidden_size_lstm, num_classes_conv)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(train_loader_lstm, train_loader_cnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader_lstm.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    combined_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for (data_lstm, _), (data_conv, labels) in zip(val_loader_lstm, val_loader_cnn):\n",
    "            outputs = combined_model(data_lstm, data_conv)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            _, predicted_val = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted_val == labels).sum().item()\n",
    "    val_loss /= len(val_loader_lstm.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Testing\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for (data_lstm, _), (data_conv, labels) in zip(test_loader_lstm, test_loader_cnn):\n",
    "        outputs = combined_model(data_lstm, data_conv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "        _, predicted_test = torch.max(outputs, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted_test == labels).sum().item()\n",
    "test_loss /= len(test_loader_lstm.dataset)\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
